# -*- coding: utf-8 -*-
"""ReviewMining Replication.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oDQ0yc1ap1PT27tk2F4WCqPV63ijopuS

# Pre-requsitis installations and imports
"""

!pip install pyspellchecker
!pip install contractions
!pip install chart_studio
!pip install varclushi
!pip install scipy

#imports

#nltk
import nltk

import nltk
nltk.download('wordnet')

import nltk
nltk.download('stopwords')

import nltk
nltk.download('punkt')

import nltk
nltk.download('words')

import nltk
from nltk.stem import WordNetLemmatizer

import math
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import chart_studio.plotly as py
import plotly.graph_objects as go
import re, string, unicodedata
import contractions
import inflect
from bs4 import BeautifulSoup
from nltk import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import LancasterStemmer, WordNetLemmatizer, PorterStemmer
from nltk.stem.snowball import SnowballStemmer
import re
from nltk import tokenize
from spellchecker import SpellChecker
from nltk.tokenize.toktok import ToktokTokenizer
import csv
import json
import string
import urllib.request
import os
from sklearn.feature_extraction.text import TfidfVectorizer 
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
from string import punctuation
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import pandas as pd
from numpy.random import seed
from numpy.random import randn
from scipy.stats import mannwhitneyu
import subprocess
import shlex
import os.path
import sys
import datetime

"""# Data load"""

!cd /content/
!sudo apt-get install wget
### file with 4 apps !!!!!! processing time is shorter because of limited number of apps
!wget https://www.dropbox.com/s/sle6ze8j4cjoqv5/Replication.csv
### whole apps !!!!!! processing time very long
#!wget https://www.dropbox.com/s/sle6ze8j4cjoqv5/Replication.csv

def dl_load_data(dl=True):
  if dl:
    url = "https://www.dropbox.com/sh/dm4vloxybhkx9o4/AACgytH1LA1Ft5FiPzKRLnuPa/Reviews%20and%20ratings?dl=0"
    !wget -O Reviews_and_ratings.zip $url
    !mkdir -p data
    !cd data && unzip -qo ../Reviews_and_ratings.zip
    !rm -rf Reviews_and_ratings.zip
  review_file = 'appbot-reviews.csv'
  reviews_file = !find -name $review_file
  return pd.concat([pd.read_csv(reviews) for reviews in reviews_file ])

df_reviews = dl_load_data(dl=True)

"""This is the group replication project of Maryam Abedy and Masoumeh Nourollahi.
In this project we have replicated NOEI paper. You can find our class [presentation](https://drive.google.com/file/d/14T3XHtpQXG7Prno9qAqtNALqCW82Iw5W/view?usp=sharing) and the paper at [NOEI](https://drive.google.com/file/d/16fPlQlGIc75Q8rt65vnB-Iyi5Hnd_xLl/view?usp=sharing) links.
"""

print('Number of test apps:')
print(len(unique(df_reviews['App'])))

print('\nStructure of data:')
df_reviews.info()

print(df_reviews.columns.values)
df_reviews.size

"""# Experiment setup

1. Preprocessing data
2. Fetching issue data from github
3. Measuring actual issue prioritization
4. Measuring issue metrics
5. Measuring user-review metrics

##1. Preprocessing data

1. Removing uninformative or non-English user-reviews User Reviews
2. Correcting Typos
3. Resolving Synonyms
4. Resolving Negations
5. Removing Stop-words 
6. Stemming
7. Extracting n–grams

###Removing uninformative or non-English user-reviews User Reviews
"""

####removing reviews that their country is not English
#After manuall analysing I learnt that reviews that their countries are not English are not in english language
df_reviews = df_reviews[df_reviews.Country == 'English']
df_reviews.size

####################
# remove non_english alphabets

def remove_nonEnglishAlphabets (text):
 words = set(nltk.corpus.words.words())
 result=" ".join(w for w in nltk.wordpunct_tokenize(text) \
        if w.lower() in words or not w.isalpha())
        
 return result+' ' 

#test 

text1 = "ではホットスポットを全く表示しない"
text2 = "i am ok ではホットスポットを全く表示しない"
print('text1:',remove_nonEnglishAlphabets (text1))
print('text2:',remove_nonEnglishAlphabets (text2))

###########Removing accented characters
import unicodedata
def remove_accented_chars(text):
    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
    return text

#test
remove_accented_chars('Sómě Áccěntěd těxt')

"""###Correcting Typos"""

#Replace repeated character sequences of length 3 or greater with sequence of length 3.

def reduce_lengthening(text):
    pattern = re.compile(r"(.)\1{2,}")
    return pattern.sub(r"\1\1", text)

#test
s= 'Awesomeeee! Exactlyyyyy what i was looking for! The only thing I could add'
print('reduce_lengthening(s)',reduce_lengthening(s))

def typoCorrection (text): 
  spell = SpellChecker()
  words = spell.split_words(text)
  #print([spell.correction(word) for word in words])
  #print('" ".join(words):'," ".join([spell.correction(word) for word in words]))
  return " ".join([spell.correction(word) for word in words])

#test: 
text='"this sentnce hus misspelled werds"'
print('typoCorrection (text):', typoCorrection (text))

"""###Stemming"""

def stemmer(text):
    ps = nltk.porter.PorterStemmer()
    text = ' '.join([ps.stem(word) for word in text.split()])
    return text

#test
stemmer("My system keeps crashing his crashed yesterday, ours crashes daily")

"""### Lemmitization"""

#Lemmatization

def lemmatizing(text):
    
    lemmatizer = WordNetLemmatizer()
    
    #Tokenize: Split the sentence into words
    word_list = nltk.word_tokenize(text)
    
    # Lemmatize list of words and join
    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
    return lemmatized_output

#test
lemmatizing("The striped bats are hanging on their feet for best")

"""###Removing Stop-words"""

def remove_stopwords(text, is_lower_case=False):
    stopword_list = nltk.corpus.stopwords.words('english')
    tokenizer = ToktokTokenizer()
    tokens = tokenizer.tokenize(text)
    tokens = [token.strip() for token in tokens]
    if is_lower_case:
        filtered_tokens = [token for token in tokens if token not in stopword_list]
    else:
        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)    
    return filtered_text
    
#test
remove_stopwords("The, and, if are stopwords, computer is not")

def remove_punctuation(text):
    return ''.join(c for c in text if c not in punctuation)

#test function
text = "Hello!!! how are you doing?"
print (remove_punctuation(text))

"""###Extracting n–grams"""

# define generating n-gram functions
def generate_ngrams(s, n):
    # Convert to lowercases
    s = s.lower()

    # Replace all none alphanumeric characters with spaces
    s = re.sub(r'[^a-zA-Z0-9\s]', ' ', s)

    # Break sentence in the token, remove empty tokens
    tokens = [token for token in s.split(" ") if token != ""]

    # Use the zip function to help us generate n-grams
    # Concatentate the tokens into ngrams and return
    ngrams = zip(*[tokens[i:] for i in range(n)])
    return [" ".join(ngram) for ngram in ngrams]

#Applying the above function to the sentence, with n=2,3,4 ,
# test
s= 'Awesome! Exactly what i was looking for!  The only thing I could add'
print('generate_ngrams(s, n=2)',generate_ngrams(s, n=2))
print('generate_ngrams(s, n=3)',generate_ngrams(s, n=3))
print('generate_ngrams(s, n=4)',generate_ngrams(s, n=4))

"""### Pipeline of pre-processing user-reviews"""

cleanedBody =[]
review2grams = []
review3grams = []
review4grams = []
ratings=[]
i = 0
###### $$$$$$$$$$$$$$$$$$ we take only first 100 column to test the application
df_reviews_test=df_reviews.head(20)
#df_reviews_test=df_reviews.head(6819)
for row in range(len(df_reviews_test)):
  body = df_reviews_test.iloc[row]["Body"]
  rating = df_reviews_test.iloc[row]["Rating"]
  body = str(body).lower()
  rating=str(rating).lower()
  #print('Body before cleaning:\n',body)
  #Apply preprocessing functions
  #1
  result_removed_stopwords=remove_stopwords(body)
  #2
  result_removed_punctuation=remove_punctuation(result_removed_stopwords)
  #3
  result_stemmed =stemmer(result_removed_punctuation)
  #4
  result_lemmatized=lemmatizing(result_stemmed)
  #5
  result_reduced_length= reduce_lengthening(result_lemmatized)
  #6
  result_typoCorrected= typoCorrection(result_reduced_length)
  #7
  result_removedNonEnglishAlphabets=remove_nonEnglishAlphabets(result_typoCorrected)
  #8
  result_remove_accented_chars = remove_accented_chars(result_removedNonEnglishAlphabets)
  
  #9
  result_2grams = generate_ngrams(result_remove_accented_chars,2) 
  result_3grams = generate_ngrams(result_remove_accented_chars,3)
  result_4grams = generate_ngrams(result_remove_accented_chars,4)
  
  i = i+1
#  print('Final result:\n',result_remove_accented_chars)
  cleanedBody.append(result_remove_accented_chars)
  review2grams.append(result_2grams)
  review3grams.append(result_3grams)
  review4grams.append(result_4grams)
  ratings.append(rating)


df_reviews_test['cleanedBody'] = cleanedBody
df_reviews_test['review2grams']= review2grams
df_reviews_test['review3grams']= review3grams
df_reviews_test['review4grams']= review4grams
df_reviews_test['strRating']= ratings

df_reviews_test.head(5)

"""## General Calculations

###Arithmatic calculations
"""

# 4, 5 measuring user and issue metrics
# code to calculate the required statictics
# this code calculates mean, median, minimum, maximum, 1st quartile, and 3rd quartile of each metric

class Statistics():

    def __init__(self):

        """
        Simply create a set of attributes with default values.
        """

        self.data = []
        self.count = 0
        self.total = 0
        self.arithmetic_mean = 0
        self.minimum = 0
        self.lower_quartile = 0
        self.median = 0
        self.upper_quartile = 0
        self.maximum = 0


    def __is_even(self, n):
        return n % 2 == 0


    def calculate(self):

        """
        Calculate statistics from data.
        Individual calculations are described in comments.
        """

        sum_of_squares = 0;
        lower_quartile_index_1 = 0
        lower_quartile_index_2 = 0

        # data needs to be sorted for median etc
        

        # count is just the size of the data set
        self.count = len(self.data)

        # initialize total to 0, and then iterate data
        # calculating total and sum of squares
        self.total = 0
        for i in self.data:
            self.total += i
            sum_of_squares += i ** 2

        # the arithmetic mean is simply the total divided by the count
        self.arithmetic_mean = self.total / self.count

        # method of calculating median and quartiles is different for odd and even count
        if self.__is_even(self.count):

            self.median = int((self.data[int(((self.count) / 2) - 1)] + self.data[int(self.count / 2)]) / 2)


            if self.__is_even(self.count / 2): # even / even

                lower_quartile_index_1 = int((self.count / 2) / 2)
                lower_quartile_index_2 = lower_quartile_index_1 - 1

                self.lower_quartile = (self.data[int(lower_quartile_index_1)] + self.data[int(lower_quartile_index_2)]) / 2
                self.upper_quartile = (self.data[int(self.count - 1 - lower_quartile_index_1)] + self.data[int(self.count - 1 - lower_quartile_index_2)]) / 2

            else: # even / odd

                lower_quartile_index_1 = int(((self.count / 2) - 1) / 2)

                self.lower_quartile = self.data[lower_quartile_index_1]
                self.upper_quartile = self.data[self.count - 1 - lower_quartile_index_1]

        else:

            self.median = int(self.data[int((self.count + 1) / 2) - 1])

            if self.__is_even((self.count - 1) / 2): # odd / even
                lower_quartile_index_1 = int(((self.count - 1) / 2) / 2)
                lower_quartile_index_2 = lower_quartile_index_1 - 1

                self.lower_quartile = (self.data[lower_quartile_index_1] + self.data[lower_quartile_index_2]) / 2
                if self.count == 1:
                  self.upper_quartile=self.data[0]
                else:
                  self.upper_quartile = (self.data[self.count - 1 - lower_quartile_index_1] + self.data[self.count - 1 - lower_quartile_index_2]) / 2

            else: # odd / odd
                lower_quartile_index_1 = int((((self.count - 1) / 2) - 1) / 2)

                self.lower_quartile = self.data[lower_quartile_index_1]
                if self.count == 1:
                  self.upper_quartile=self.data[0]
                else:
                  self.upper_quartile = self.data[self.count - 1 - lower_quartile_index_1]

        # the data is sorted so the mimimum and maximum are the first and last values
        self.minimum = self.data[0]
        self.maximum = self.data[self.count - 1]

"""### datetime difference calculation"""

import datetime

def try_strptime(s, format):
    """
    '%Y, %m, %d'
    @param s the string to parse
    @param format the format to attempt parsing of the given string
    @return the parsed datetime or None on failure to parse 
    @see datetime.datetime.strptime
    """
    try:
        date = datetime.datetime.strptime(s, format)
    except ValueError:
        date = None
    return date

"""### Unique list"""

def unique(list1): 
  
    # intilize a null list 
    unique_list = [] 
      
    # traverse for all elements 
    for x in list1: 
        # check if exists in unique_list or not 
        if x not in unique_list: 
            unique_list.append(x) 
    return unique_list

"""##Fetching issue data from github and calculating issue GQM metrics

###Fetching issue data & issues and reporters GQM metrics

fields **2. Fetching issue data from github**
in this section 3 types of data (issue, user and comment data) are fetched from gitgub. 
some of the fields required for GQM metrics are directly used from these outputs or are result
of some calculations on this data

this process is repeated for all of the apps under test

for each issue in the issues page there is a "number" field. To get details of each issue we should got to this url:

https://api.github.com/repos/tensorflow/tensorflow/issues/[“number”]/[comments]

    • number of users contributed to resolving an issue  ---> for one issue number how many contributor tags /// or it can be number of records in "assignees"
    • number of comments on each issue report: "comments"
    • size of issue report  --> sizeof(body)
    • size of comments --> sizeof(
    • reporter contribution (based on contribution score of github):  https://help.github.com/en/github/setting-up-and-managing-your-github-profile/viewing-contributions-on-your-profile
    • contribution of users who have involved in issues: same as above

all of these parameters are fetched from: “url": "https://api.github.com/users/["login"]” in "user"
    • time since reporter joined github: "created_at" 
    • time since contributors have joined github:  "created_at" for each of the contributors
    • number of following and followers of reporters: "followers"
    • number of following and followers of contributors: "following"
    • number of gists of reporters:  "public_gists"
    • number of gists of contributors: "public_gists" for each of the contributors
    • number of public repositories of reporters: "public_repos"
    • number of public repositories of contributors: "public_repos" for each of the contributors
      
each issue report also has:

    • title: "title"
    • issue number:  "number"
    • body: "body"
    • status:  "state"
    • open date: "created_at"
    • close date: "closed_at"
    • reporter: "login"
    • contributor --> when "author_association": "CONTRIBUTOR"
"""

#2. Fetching issue data from github
import random

number_of_users= []
sizes_of_issue_reports_title_words= []
sizes_of_issue_reports_body_words= []
sizes_of_issue_reports_body_sentences= []
duration_reporter_join_github= []
reporter_number_of_following= []
reporter_number_of_followers= []
number_of_reporter_gists_mean= []
number_of_public_repos_reporter= []
l_date = datetime.datetime.now()

reporters=[]
#accessTokenCounter = 0
#pageCounter = 0

accessToken = ["83edd229a49797d1733e09381b3a23a1f94b878f",
               "0bba3b6ed7410710361d0c8cd180b80929876c2f",
               "e1b83bdf6f7c869c625cc1f3f398e84edc882608",
               "d398f5eee4b2f8b561f334b857428361b99bdb3d",
               "eec2bca621f35b0d9005ef22db32ceadfe1a61a2",
               "85327aa7ba46ac4f8873734b3898f5a6afb4901c",
               "feb98526013f65ee89b4d679076ef62a9d81c42a",
               "05328f588ef5aa0199e2c0badf94ffe1a8ad8813"]

path=  '/content/Replication.csv'
# we need to put a for loop to iterate over all links in the shared excell files of github urls
# reading from CSV
# we need to put a for loop to iterate over all links in the shared excell files of github urls

with open(path) as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    apps = []
    repos = []
    repoPaths= []
    appIDs=[]
    avgRatings=[]
    priorities=[]
  
    for row in readCSV:
      app = row[0]
      repo = row[6]
      repoPath=row[7]
      appID= row[8]
      apps.append(app)
      appIDs.append(appID)
      repos.append(repo)
      repoPaths.append(repoPath)
      flag = True
      accessTokenCounter = 0
      pageCounter = 0

      avgRating= row[1]
      avgRatings.append(avgRating)


      with open(appID + '.csv', 'w', encoding="utf-8", newline='') as csvOut:
        writer = csv.writer(csvOut)

        while flag==True:
          url = repo + "issues?state=all&per_page=100&page="+ str(pageCounter) + "&access_token=" + accessToken[(accessTokenCounter % 8)]
          issues = urllib.request.urlopen(url).read()
          parsData = json.loads(issues)
          """
          strToPrint.append('repoPath')
          strToPrint.append('IssueNumber')
          strToPrint.append('title')
          strToPrint.append('body')
          strToPrint.append('created_at')
          strToPrint.append('closed_at')
          strToPrint.append('user')
          strToPrint.append('comments')
          writer.writerow(strToPrint)
          """
          for issue in parsData:
            strToPrint = []
            strToPrint.append(repoPath)
            strToPrint.append(issue['number'])
            strToPrint.append(issue['title'])
            strToPrint.append(issue['body'])
            strToPrint.append(issue['created_at'])
            strToPrint.append(issue['closed_at'])
            reporters.append(issue['user']['login'])
            strToPrint.append(issue['user']['login'])
            strToPrint.append(issue['comments'])
            randnumber= random.randint(0, 3) 
            priorities.append(randnumber)
            writer.writerow(strToPrint)    

            issueSenBodySize = len(tokenize.sent_tokenize(str(issue['body'])))
            issueWordBodySize = len(tokenize.word_tokenize(str(issue['body'])))
            sizes_of_issue_reports_body_sentences.append(issueSenBodySize)
            sizes_of_issue_reports_body_words.append(issueWordBodySize)
            issueWordTitleSize = len(tokenize.word_tokenize(str(issue['title'])))
            sizes_of_issue_reports_title_words.append(issueWordTitleSize)   

            urlUser = "https://api.github.com/users/" + str(issue['user']['login'])+"?state=all"+ "&access_token=05328f588ef5aa0199e2c0badf94ffe1a8ad8813"   
            reportersList = urllib.request.urlopen(urlUser).read()
            parsReporterData = json.loads(reportersList)

            f_date = try_strptime(str(parsReporterData['created_at']), '%Y-%m-%dT%H:%M:%SZ')
            delta = l_date - f_date
            duration_reporter_join_github.append(delta.days)
            reporter_number_of_followers.append(parsReporterData['followers'])
            reporter_number_of_following.append(parsReporterData['following'])
            number_of_reporter_gists_mean.append(parsReporterData['public_gists'])
            number_of_public_repos_reporter.append(parsReporterData['public_repos'])       
          
          if not parsData:
            flag = False
            break
  
          accessTokenCounter = accessTokenCounter + 1
          pageCounter = pageCounter + 1
         
        csvOut.close()
    
    issueGQM = pd.DataFrame(number_of_users, columns=['number_of_users'])
    issueGQM['sizes_of_issue_reports_title_words']=  pd.Series(sizes_of_issue_reports_title_words)
    issueGQM['sizes_of_issue_reports_body_words']=  pd.Series(sizes_of_issue_reports_body_words)
    issueGQM['sizes_of_issue_reports_body_sentences']=  pd.Series(sizes_of_issue_reports_body_sentences)
    issueGQM['duration_reporter_join_github']=  pd.Series(duration_reporter_join_github)
    issueGQM['reporter_number_of_following']=  pd.Series(reporter_number_of_following)
    issueGQM['reporter_number_of_followers']=  pd.Series(reporter_number_of_followers)
    issueGQM['number_of_reporter_gists_mean']=  pd.Series(number_of_reporter_gists_mean)
    issueGQM['number_of_public_repos_reporter']=  pd.Series(number_of_public_repos_reporter)
    issueGQM['IssuePriority']= pd.Series(priorities)

    issueGQM.to_csv('IssueGQM'+appID+'.csv', sep=',', encoding='utf-8')

"""###Fetching comments data & comments and contributors GQM metrics"""

import statistics

s=Statistics()

contributors=[]
number_of_users= []
number_of_comments= []
sizes_of_comments_sentences_mean= []
sizes_of_comments_sentences_median= []
sizes_of_comments_sentences_min= []
sizes_of_comments_sentences_max= []
sizes_of_comments_sentences_1q= []
sizes_of_comments_sentences_3q= []
sizes_of_comments_words_mean= []
sizes_of_comments_words_median= [] 
sizes_of_comments_words_min= []
sizes_of_comments_words_max= []
sizes_of_comments_words_1q= []
sizes_of_comments_words_3q= []

duration_contributor_join_github_mean= []
duration_contributor_join_github_median= []
duration_contributor_join_github_min= []
duration_contributor_join_github_max= []
duration_contributor_join_github_1q= []
duration_contributor_join_github_3q= []
contributor_number_of_following_mean= []
contributor_number_of_following_median= []
contributor_number_of_following_min= []
contributor_number_of_following_max= []
contributor_number_of_following_1q= []
contributor_number_of_following_3q= []
contributor_number_of_followers_mean= []
contributor_number_of_followers_median= []
contributor_number_of_followers_min= []
contributor_number_of_followers_max= []
contributor_number_of_followers_1q= []
contributor_number_of_followers_3q= []

number_of_contributor_gists_mean= []
number_of_contributor_gists_median= [] 
number_of_contributor_gists_min= []
number_of_contributor_gists_max= []
number_of_contributor_gists_1q= []
number_of_contributor_gists_3q= [] 

number_of_public_repos_contributor_mean= []
number_of_public_repos_contributor_median= []
number_of_public_repos_contributor_min= []
number_of_public_repos_contributor_max= []
number_of_public_repos_contributor_1q= [] 
number_of_public_repos_contributor_3q= []

l_date = datetime.datetime.now()

for appID in appIDs:
  path= '/content/'+appID+'.csv'
  
  with open(path) as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    issueNumbers = []

    with open(appID+'IssueComments.csv', 'w', encoding="utf-8", newline='') as csvOut:
      writer = csv.writer(csvOut)
      
      for row in readCSV:
        issueNumber = str(row[1])
        numberOfComments = int(row[7])
        repository=row[0]
        users=[]
        commentSenBodySizes=[]
        commentWordBodySizes=[]
        duration_contributor_join_github=[]
        numberOfFollowers=[]
        numberOfFollowing=[]
        numberOfPublicGists=[]
        numberOfPublicRepos=[]
        
        pageCounter=0
        flag=True
        
        if numberOfComments != 0:
          while flag==True:
            urlComment = "https://api.github.com/repos/"+repository + "issues/" + issueNumber + "/comments"+"?state=all&per_page=100&page="+ str(pageCounter) + "&access_token=" + accessToken[(accessTokenCounter % 8)]
            comments = urllib.request.urlopen(urlComment).read()
            commentParsData = json.loads(comments)

            number_of_comments.append(numberOfComments)
            
            for comment in commentParsData:
              strToPrint = []
              strToPrint.append(issueNumber)
              strToPrint.append(comment['body'])
              strToPrint.append(comment['created_at'])
              strToPrint.append(comment['updated_at'])
              contributors.append(str(comment['user']['login']))
              writer.writerow(strToPrint)
            
              commentSenBodySize= len(tokenize.sent_tokenize(comment['body']))
              commentSenBodySizes.append(float(commentSenBodySize))
              commentWordBodySize= len(tokenize.word_tokenize(comment['body']))
              commentWordBodySizes.append(float(commentWordBodySize))

              users.append(str(comment['user']['login']))

              urlUser = "https://api.github.com/users/" + str(comment['user']['login'])+"?state=all"+ "&access_token="+ accessToken[(accessTokenCounter % 8)]
              contributorsList = urllib.request.urlopen(urlUser).read()
              parsContributorData = json.loads(contributorsList)

              f_date = try_strptime(parsContributorData['created_at'], '%Y-%m-%dT%H:%M:%SZ')
              delta = l_date - f_date
              duration_contributor_join_github.append(delta.days)
              numberOfFollowers.append(float(parsContributorData['followers']))
              numberOfFollowing.append(float(parsContributorData['following']))
              numberOfPublicGists.append(float(parsContributorData['public_gists']))
              numberOfPublicRepos.append(float(parsContributorData['public_repos']))

                          
            if not parsData:
              flag = False
              break

            accessTokenCounter = accessTokenCounter + 1
            pageCounter = pageCounter + 1
          
          # The number of GitHub users who have involved in the discussions can implicitly show the importance of an issue.           
          users=unique(users)
          number_of_users.append(len(users))

        if len(commentSenBodySizes) != 0:
          data= commentSenBodySizes
          s.data = data
          data=data.sort()
          s.calculate()
          commentSenBodySizessArithmathy= [str(s.count), str(s.total), str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile), str(s.median), str(s.upper_quartile), str(s.maximum)]            
        
          sizes_of_comments_sentences_mean.append(float(commentSenBodySizessArithmathy[2])) #sizes_of_comments_sentences_mean        
          sizes_of_comments_sentences_median.append(float(commentSenBodySizessArithmathy[5])) #sizes_of_comments_sentences_median
          sizes_of_comments_sentences_min.append(float(commentSenBodySizessArithmathy[3])) #sizes_of_comments_sentences_min
          sizes_of_comments_sentences_max.append(float(commentSenBodySizessArithmathy[7])) #sizes_of_comments_sentences_max
          sizes_of_comments_sentences_1q.append(float(commentSenBodySizessArithmathy[4])) #sizes_of_comments_sentences_1q
          sizes_of_comments_sentences_3q.append(float(commentSenBodySizessArithmathy[6])) #sizes_of_comments_sentences_3q

        if len(commentWordBodySizes) != 0:
          data= commentWordBodySizes
          s.data = data
          data=data.sort()
          s.calculate() 
            
          commentWordBodySizessArithmathy= [str(s.count), str(s.total), str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile), str(s.median), str(s.upper_quartile), str(s.maximum)]

          sizes_of_comments_words_mean.append(float(commentWordBodySizessArithmathy[2])) #sizes_of_comments_words_mean
          sizes_of_comments_words_median.append(float(commentWordBodySizessArithmathy[5])) #sizes_of_comments_words_median
          sizes_of_comments_words_min.append(float(commentWordBodySizessArithmathy[3])) #sizes_of_comments_words_min
          sizes_of_comments_words_max.append(float(commentWordBodySizessArithmathy[7])) #sizes_of_comments_words_max
          sizes_of_comments_words_1q.append(float(commentWordBodySizessArithmathy[4])) #sizes_of_comments_words_1q
          sizes_of_comments_words_3q.append(float(commentWordBodySizessArithmathy[6])) #sizes_of_comments_words_3q

          data= duration_contributor_join_github
          s.data = data
          data=data.sort()
          s.calculate() 

          contributorsDurationsArithmathy= [str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]
             
          duration_contributor_join_github_mean.append(float(contributorsDurationsArithmathy[0])) #duration_contributor_join_github_mean= 0
          duration_contributor_join_github_median.append(float(contributorsDurationsArithmathy[3])) #duration_contributor_join_github_median= 0
          duration_contributor_join_github_min.append(float(contributorsDurationsArithmathy[1])) #duration_contributor_join_github_min= 0
          duration_contributor_join_github_max.append(float(contributorsDurationsArithmathy[5])) #duration_contributor_join_github_max= 0
          duration_contributor_join_github_1q.append(float(contributorsDurationsArithmathy[2])) #duration_contributor_join_github_1q= 0
          duration_contributor_join_github_3q.append(float(contributorsDurationsArithmathy[4])) #duration_contributor_join_github_3q= 0

          # We measure the number of followers and the number of followings of the users who have contributed to each issue report
          data= numberOfFollowing
          s.data = data
          data=data.sort()
          s.calculate() 

          contributorsNumberOfFollowingArithmathy= [str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]

          contributor_number_of_following_mean.append(float(contributorsNumberOfFollowingArithmathy[0])) #contributor_number_of_following_mean= 0
          contributor_number_of_following_median.append(float(contributorsNumberOfFollowingArithmathy[3])) #contributor_number_of_following_median= 0
          contributor_number_of_following_min.append(float(contributorsNumberOfFollowingArithmathy[1])) #contributor_number_of_following_min= 0
          contributor_number_of_following_max.append(float(contributorsNumberOfFollowingArithmathy[5])) #contributor_number_of_following_max= 0
          contributor_number_of_following_1q.append(float(contributorsNumberOfFollowingArithmathy[2])) #contributor_number_of_following_1q= 0
          contributor_number_of_following_3q.append(float(contributorsNumberOfFollowingArithmathy[4])) #contributor_number_of_following_3q= 0

          data= numberOfFollowers
          s.data = data
          data=data.sort()
          s.calculate() 

          contributorsNumberOfFollowersArithmathy= [str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]

          contributor_number_of_followers_mean.append(float(contributorsNumberOfFollowersArithmathy[0])) #contributor_number_of_followers_mean= 0
          contributor_number_of_followers_median.append(float(contributorsNumberOfFollowersArithmathy[3])) #contributor_number_of_followers_median= 0
          contributor_number_of_followers_min.append(float(contributorsNumberOfFollowersArithmathy[1])) #contributor_number_of_followers_min= 0
          contributor_number_of_followers_max.append(float(contributorsNumberOfFollowersArithmathy[5])) #contributor_number_of_followers_max= 0
          contributor_number_of_followers_1q.append(float(contributorsNumberOfFollowersArithmathy[2])) #contributor_number_of_followers_1q= 0
          contributor_number_of_followers_3q.append(float(contributorsNumberOfFollowersArithmathy[4])) #contributor_number_of_followers_3q= 0

          # measure the number of gists of the users who have contributed to an issue report
          data= numberOfPublicGists
          s.data = data
          data=data.sort()
          s.calculate() 

          contributorsNumberOfPublicGistsArithmathy= [str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]
             
          number_of_contributor_gists_mean.append(float(contributorsNumberOfPublicGistsArithmathy[0])) #number_of_contributor_gists_mean= 0
          number_of_contributor_gists_median.append(float(contributorsNumberOfPublicGistsArithmathy[3])) #number_of_contributor_gists_median= 0
          number_of_contributor_gists_min.append(float(contributorsNumberOfPublicGistsArithmathy[1])) #number_of_contributor_gists_min= 0
          number_of_contributor_gists_max.append(float(contributorsNumberOfPublicGistsArithmathy[5])) #number_of_contributor_gists_max= 0
          number_of_contributor_gists_1q.append(float(contributorsNumberOfPublicGistsArithmathy[2])) #number_of_contributor_gists_1q= 0
          number_of_contributor_gists_3q.append(float(contributorsNumberOfPublicGistsArithmathy[4])) #number_of_contributor_gists_3q= 0

          # count the number public repositories of the contributors to an issue report
          data= numberOfPublicRepos
          s.data = data
          data=data.sort()
          s.calculate() 

          contributorsNumberOfPublicReposArithmathy= [str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]

          number_of_public_repos_contributor_mean.append(float(contributorsNumberOfPublicReposArithmathy[0])) #number_of_public_repos_contributor_mean= 0
          number_of_public_repos_contributor_median.append(float(contributorsNumberOfPublicReposArithmathy[3])) #number_of_public_repos_contributor_median= 0
          number_of_public_repos_contributor_min.append(float(contributorsNumberOfPublicReposArithmathy[1])) #number_of_public_repos_contributor_min= 0
          number_of_public_repos_contributor_max.append(float(contributorsNumberOfPublicReposArithmathy[5])) #number_of_public_repos_contributor_max= 0
          number_of_public_repos_contributor_1q.append(float(contributorsNumberOfPublicReposArithmathy[2])) #number_of_public_repos_contributor_1q= 0
          number_of_public_repos_contributor_3q.append(float(contributorsNumberOfPublicReposArithmathy[4])) #number_of_public_repos_contributor_3q= 0


    csvOut.close()

    issueGQM['number_of_commnts'] = pd.Series(number_of_comments)
    issueGQM['sizes_of_comments_sentences_mean'] = pd.Series(sizes_of_comments_sentences_mean)
    issueGQM['sizes_of_comments_sentences_min'] = pd.Series(sizes_of_comments_sentences_min)
    issueGQM['sizes_of_comments_sentences_max'] = pd.Series(sizes_of_comments_sentences_max) 
    issueGQM['sizes_of_comments_sentences_1q'] = pd.Series(sizes_of_comments_sentences_1q)
    issueGQM['sizes_of_comments_sentences_3q'] = pd.Series(sizes_of_comments_sentences_3q)
    issueGQM['sizes_of_comments_words_mean'] = pd.Series(sizes_of_comments_words_mean)
    issueGQM['sizes_of_comments_words_median'] = pd.Series(sizes_of_comments_words_median)
    issueGQM['sizes_of_comments_words_min']= pd.Series(sizes_of_comments_words_min)
    issueGQM['sizes_of_comments_words_max'] = pd.Series(sizes_of_comments_words_max)
    issueGQM['sizes_of_comments_words_1q'] = pd.Series(sizes_of_comments_words_1q) 
    issueGQM['sizes_of_comments_sentences_median'] = pd.Series(sizes_of_comments_sentences_median)
    issueGQM['sizes_of_comments_words_3q'] = pd.Series(sizes_of_comments_words_3q)
    
    issueGQM['duration_contributor_join_github_mean']=  pd.Series(duration_contributor_join_github_mean)
    issueGQM['duration_contributor_join_github_median']=  pd.Series(duration_contributor_join_github_median)
    issueGQM['duration_contributor_join_github_min']=  pd.Series(duration_contributor_join_github_min)
    issueGQM['duration_contributor_join_github_max']=  pd.Series(duration_contributor_join_github_max)
    issueGQM['duration_contributor_join_github_1q']=  pd.Series(duration_contributor_join_github_1q)
    issueGQM['duration_contributor_join_github_3q']=  pd.Series(duration_contributor_join_github_3q)

    issueGQM['contributor_number_of_following_mean']=  pd.Series(contributor_number_of_following_mean)
    issueGQM['contributor_number_of_following_median']=  pd.Series(contributor_number_of_following_median)
    issueGQM['contributor_number_of_following_min']=  pd.Series(contributor_number_of_following_min)
    issueGQM['contributor_number_of_following_max']=  pd.Series(contributor_number_of_following_max)
    issueGQM['contributor_number_of_following_1q']=  pd.Series(contributor_number_of_following_1q)
    issueGQM['contributor_number_of_following_3q']=  pd.Series(contributor_number_of_following_3q)
    issueGQM['contributor_number_of_followers_mean']=  pd.Series(contributor_number_of_followers_mean)
    issueGQM['contributor_number_of_followers_median']=  pd.Series(contributor_number_of_followers_median)
    issueGQM['contributor_number_of_followers_min']=  pd.Series(contributor_number_of_followers_min)
    issueGQM['contributor_number_of_followers_max']=  pd.Series(contributor_number_of_followers_max)
    issueGQM['contributor_number_of_followers_1q']=  pd.Series(contributor_number_of_followers_1q)
    issueGQM['contributor_number_of_followers_3q']=  pd.Series(contributor_number_of_followers_3q)

    issueGQM['number_of_contributor_gists_mean']=  pd.Series(number_of_contributor_gists_mean)
    issueGQM['number_of_contributor_gists_median']=  pd.Series(number_of_contributor_gists_median)
    issueGQM['number_of_contributor_gists_min']=  pd.Series(number_of_contributor_gists_min)
    issueGQM['number_of_contributor_gists_max']=  pd.Series(number_of_contributor_gists_max)
    issueGQM['number_of_contributor_gists_1q']=  pd.Series(number_of_contributor_gists_1q)
    issueGQM['number_of_contributor_gists_3q']=  pd.Series(number_of_contributor_gists_3q)

    issueGQM['number_of_public_repos_contributor_mean']=  pd.Series(number_of_public_repos_contributor_mean)
    issueGQM['number_of_public_repos_contributor_median']=  pd.Series(number_of_public_repos_contributor_median)
    issueGQM['number_of_public_repos_contributor_min']=  pd.Series(number_of_public_repos_contributor_min)
    issueGQM['number_of_public_repos_contributor_max']=  pd.Series(number_of_public_repos_contributor_max)
    issueGQM['number_of_public_repos_contributor_1q']=  pd.Series(number_of_public_repos_contributor_1q)
    issueGQM['number_of_public_repos_contributor_3q']=  pd.Series(number_of_public_repos_contributor_3q)

    issueGQM.to_csv('IssueGQM'+appID+'.csv', sep=',', encoding='utf-8')

"""###Adding GQM metrics to GQM matrix"""

issueGQM = pd.DataFrame(number_of_users, columns=['number_of_users'])
#issueGQM['number_of_users'] = pd.Series(number_of_users)
issueGQM['sizes_of_issue_reports_title_words']=  pd.Series(sizes_of_issue_reports_title_words)
issueGQM['sizes_of_issue_reports_body_words']=  pd.Series(sizes_of_issue_reports_body_words)
issueGQM['sizes_of_issue_reports_body_sentences']=  pd.Series(sizes_of_issue_reports_body_sentences)

issueGQM['duration_reporter_join_github']=  pd.Series(duration_reporter_join_github)
issueGQM['reporter_number_of_following']=  pd.Series(reporter_number_of_following)
issueGQM['reporter_number_of_followers']=  pd.Series(reporter_number_of_followers)
issueGQM['number_of_reporter_gists_mean']=  pd.Series(number_of_reporter_gists_mean)
issueGQM['number_of_public_repos_reporter']=  pd.Series(number_of_public_repos_reporter)
       
issueGQM['number_of_commnts'] = pd.Series(number_of_comments)
issueGQM['sizes_of_comments_sentences_mean'] = pd.Series(sizes_of_comments_sentences_mean)
issueGQM['sizes_of_comments_sentences_min'] = pd.Series(sizes_of_comments_sentences_min)
issueGQM['sizes_of_comments_sentences_max'] = pd.Series(sizes_of_comments_sentences_max) 
issueGQM['sizes_of_comments_sentences_1q'] = pd.Series(sizes_of_comments_sentences_1q)
issueGQM['sizes_of_comments_sentences_3q'] = pd.Series(sizes_of_comments_sentences_3q)
issueGQM['sizes_of_comments_words_mean'] = pd.Series(sizes_of_comments_words_mean)
issueGQM['sizes_of_comments_words_median'] = pd.Series(sizes_of_comments_words_median)
issueGQM['sizes_of_comments_words_min']= pd.Series(sizes_of_comments_words_min)
issueGQM['sizes_of_comments_words_max'] = pd.Series(sizes_of_comments_words_max)
issueGQM['sizes_of_comments_words_1q'] = pd.Series(sizes_of_comments_words_1q) 
issueGQM['sizes_of_comments_sentences_median'] = pd.Series(sizes_of_comments_sentences_median)
issueGQM['sizes_of_comments_words_3q'] = pd.Series(sizes_of_comments_words_3q)

issueGQM['duration_contributor_join_github_mean']=  pd.Series(duration_contributor_join_github_mean)
issueGQM['duration_contributor_join_github_median']=  pd.Series(duration_contributor_join_github_median)
issueGQM['duration_contributor_join_github_min']=  pd.Series(duration_contributor_join_github_min)
issueGQM['duration_contributor_join_github_max']=  pd.Series(duration_contributor_join_github_max)
issueGQM['duration_contributor_join_github_1q']=  pd.Series(duration_contributor_join_github_1q)
issueGQM['duration_contributor_join_github_3q']=  pd.Series(duration_contributor_join_github_3q)

issueGQM['contributor_number_of_following_mean']=  pd.Series(contributor_number_of_following_mean)
issueGQM['contributor_number_of_following_median']=  pd.Series(contributor_number_of_following_median)
issueGQM['contributor_number_of_following_min']=  pd.Series(contributor_number_of_following_min)
issueGQM['contributor_number_of_following_max']=  pd.Series(contributor_number_of_following_max)
issueGQM['contributor_number_of_following_1q']=  pd.Series(contributor_number_of_following_1q)
issueGQM['contributor_number_of_following_3q']=  pd.Series(contributor_number_of_following_3q)
issueGQM['contributor_number_of_followers_mean']=  pd.Series(contributor_number_of_followers_mean)
issueGQM['contributor_number_of_followers_median']=  pd.Series(contributor_number_of_followers_median)
issueGQM['contributor_number_of_followers_min']=  pd.Series(contributor_number_of_followers_min)
issueGQM['contributor_number_of_followers_max']=  pd.Series(contributor_number_of_followers_max)
issueGQM['contributor_number_of_followers_1q']=  pd.Series(contributor_number_of_followers_1q)
issueGQM['contributor_number_of_followers_3q']=  pd.Series(contributor_number_of_followers_3q)

issueGQM['number_of_contributor_gists_mean']=  pd.Series(number_of_contributor_gists_mean)
issueGQM['number_of_contributor_gists_median']=  pd.Series(number_of_contributor_gists_median)
issueGQM['number_of_contributor_gists_min']=  pd.Series(number_of_contributor_gists_min)
issueGQM['number_of_contributor_gists_max']=  pd.Series(number_of_contributor_gists_max)
issueGQM['number_of_contributor_gists_1q']=  pd.Series(number_of_contributor_gists_1q)
issueGQM['number_of_contributor_gists_3q']=  pd.Series(number_of_contributor_gists_3q)

issueGQM['number_of_public_repos_contributor_mean']=  pd.Series(number_of_public_repos_contributor_mean)
issueGQM['number_of_public_repos_contributor_median']=  pd.Series(number_of_public_repos_contributor_median)
issueGQM['number_of_public_repos_contributor_min']=  pd.Series(number_of_public_repos_contributor_min)
issueGQM['number_of_public_repos_contributor_max']=  pd.Series(number_of_public_repos_contributor_max)
issueGQM['number_of_public_repos_contributor_1q']=  pd.Series(number_of_public_repos_contributor_1q)
issueGQM['number_of_public_repos_contributor_3q']=  pd.Series(number_of_public_repos_contributor_3q)
issueGQM['IssuePriority']= pd.Series(priorities)

issueGQM.info()

issueGQM.head(5)

contributors=unique(contributors)
print(contributors)

"""##Measuring actual issue prioritization

**!!!Warning!!!!**

 **This part does not work!**

To estimate developers’ reaction to each issue, we consider the following actions: (i) post comments on an issue report, (ii) submit commits for an issue report, and (iii) adding specific keywords to an issue report, including “Fixed”, “Solved”, “Resolved”, “Closed”, “Feature added”, and “Finished”. To measure the reaction time for each issue, we compute the minimum value of the intervals between each of the aforementioned actions and the time since an issue report has been posted on GitHub. We use the
reaction times to measure the prioritization orders of issue reports.

We were unable to fetch all the mentioned data for issues, however we could successfully fetch following from issue timeline API which we think is an appropraite replacement.

https://developer.github.com/v3/issues/timeline/

events:
added_to_project
commented
committed
closed
converted_note_to_issue
assigned
removed_from_project
reopened
review_requested
merged
marked_as_duplicate

Event Attributes:
created_at
project_card (A project_card object that includes the id, url, project_id, project_url)


GET /repos/:owner/:repo/issues/:issue_number/timeline

There is this link for getting committ histories

https://api.github.com/repos/jroal/a2dpvolume/commits
"""

import urllib.request

accessToken = ["83edd229a49797d1733e09381b3a23a1f94b878f",
               "0bba3b6ed7410710361d0c8cd180b80929876c2f",
               "e1b83bdf6f7c869c625cc1f3f398e84edc882608",
               "d398f5eee4b2f8b561f334b857428361b99bdb3d",
               "eec2bca621f35b0d9005ef22db32ceadfe1a61a2",
               "85327aa7ba46ac4f8873734b3898f5a6afb4901c",
               "feb98526013f65ee89b4d679076ef62a9d81c42a",
               "05328f588ef5aa0199e2c0badf94ffe1a8ad8813"]

url = 'https://api.github.com/repos/jroal/a2dpvolume/issues/273'"&access_token="+ accessToken[(accessTokenCounter % 4)]

header={'Accept', 'application/vnd.github.mockingbird-preview'}
req = urllib.request.Request(url, headers=header, method='DELETE')
res = urllib.request.urlopen(req)

print(res)


        events:
added_to_project
commented
committed
closed
converted_note_to_issue
assigned
removed_from_project
reopened
review_requested
merged
marked_as_duplicate

Event Attributes:
created_at
project_card (A project_card object that includes the id, url, project_id, project_url)

# 3 measuring actual issue prioritization
# in comments
created_at=""

# commit dates in data files
comitt_date= ""

# adding specific keywords to an issue report, including “Fixed”, “Solved”, “Resolved”, “Closed”, “Feature added”, and “Finished”.
keyword_add_date= ""

#priority metrics
min_reaction_time= 0

repos = []

"""For step 4,5 following actions are required:

Metrics for user review and issue reports are calculated by use of data fetched from issues in git-hub and also data provided in dataset for user-reviews

for some of the metrics we need to calculate mean, median,
minimum, maximum, 1st quartile, and 3rd quartile. Also for some others sentiment analysis was required. senti-strength tool was used in this case.
Apart from all this some of the metrics like number of contributors were calculated from data fetched from issue reports.

# RQ1

1. Apply TF-IDF (Salton and Mcgill 1983) to get vector of each review
2. Appy cosine similarity between all reviews of each app (vector space model (Salton et al. 1975)
3. Apply DBSCAN (Ester et al. 1996) to cluster reviews based on distance. DBSCAN requires two
parameters: (i) the maximum distance between user-reviews, (put 0.6), ii) the minimum number
of user-reviews that can be clustered together, that they put 1.
4. consider each cluster of reviews as one document
5. Apply TF-IDF (Salton and Mcgill 1983) on issue report
6. Compute cosine similarity between each document and all issue-report vectors
7. if similarity is more than the defined threshold reviews and issue are related to each other (0.85)

**Result:**
In their experiment the authors we able to matched 27% of the user-reviews with the 33% of the issue
reports. Using this mapping method, the user reviews can be mapped to issue reports with 79% precision

to clarify the steps in RQ1:
1. we parse the issue reviews and make a CSV file of issue reviews for each app
2. we should apply TF-IDF on each review and issue report to get the vector of the review
3. we calculate cosine similarities between issue reports and each cluster of user-reviews
4. DBSCAN algorithm is used to cluster the reviews based on the similarities calculated in the second step

##DBSCAN to cluster reviews based on distance
"""

#DBSCAN
# #############################################################################
# Compute DBSCAN
def dbscan(similarity):
  db = DBSCAN(eps=0.6, metric= 'cosine', min_samples=1).fit(similarity)
  #db=DBSCAN(eps=0.6, metric= similarity, min_samples=1)
  core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
  core_samples_mask[db.core_sample_indices_] = True
  labels = db.labels_
  
  # Number of clusters in labels, ignoring noise if present.
  n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
  n_noise_ = list(labels).count(-1)
  
  print('Estimated number of clusters: %d' % n_clusters_)
  # #############################################################################
  # Plot result
  # Black removed and is used for noise instead.

  """
  unique_labels = set(labels)
  colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
  for k, col in zip(unique_labels, colors):
    if k == -1:
      # Black used for noise.
      col = [0, 0, 0, 1]

    class_member_mask = (labels == k)
    xy = cosine[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14)

    xy = cosine[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

  plt.title('Estimated number of clusters: %d' % n_clusters_)
  plt.show()
  """

  return labels

"""##Apply TF-IDF on each review, cosine similarity between all reviews of each app, and cluster by DBSCAN"""

# Apply TFIDF on app reviews, 2gram, 3gram, 4 gram
# cluster by DBSCAN with cosine metric

for appID in appIDs:
  reviewPath= '/content/data/'+appID+'/appbot-reviews.csv'
  
  with open(reviewPath) as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    df=pd.read_csv(csvfile)

    i=0
    cleanedBody =[]
    review2grams = []
    review3grams = []
    review4grams = []
    Tfidf_values=[]
    Tfidf_2gram_values=[]
    Tfidf_3gram_values=[]
    Tfidf_4gram_values=[]

    for row in range(len(df)):
      body = df.iloc[row]['Body']
      body = str(body).lower()
      
      #Apply preprocessing functions
      #1
      result_removed_stopwords=remove_stopwords(body)
      #2
      result_removed_punctuation=remove_punctuation(result_removed_stopwords)
      #3
      result_stemmed =stemmer(result_removed_punctuation)
      #4
      result_lemmatized=lemmatizing(result_stemmed)
      #5
      result_reduced_length= reduce_lengthening(result_lemmatized)
      #6
      result_typoCorrected= typoCorrection(result_reduced_length)
      #7
      result_removedNonEnglishAlphabets=remove_nonEnglishAlphabets(result_typoCorrected)
      #8
      result_remove_accented_chars = remove_accented_chars(result_removedNonEnglishAlphabets)  
      #9
      result_2grams = generate_ngrams(result_remove_accented_chars,2) 
      result_3grams = generate_ngrams(result_remove_accented_chars,3)
      result_4grams = generate_ngrams(result_remove_accented_chars,4)
  
      i = i+1

      result_2grams= " ".join(result_2grams)
      result_3grams= " ".join(result_3grams)
      result_4grams= " ".join(result_4grams)

      cleanedBody.append(result_remove_accented_chars)
      review2grams.append(result_2grams)
      review3grams.append(result_3grams)
      review4grams.append(result_4grams)
     

    df['cleanedBody'] = cleanedBody
    df['review2grams']= review2grams
    df['review3grams']= review3grams
    df['review4grams']= review4grams

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_values=tfidf_vectorizer.fit_transform(cleanedBody)
    cls = dbscan(Tfidf_values)
    df['Tfidf_values']= Tfidf_values
    df['cls']= cls

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_2gram_values=tfidf_vectorizer.fit_transform(review2grams)
    cls2 = dbscan(Tfidf_2gram_values)    
    df['Tfidf_2gram_values']= Tfidf_2gram_values
    df['cls2']= cls2

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_3gram_values=tfidf_vectorizer.fit_transform(review3grams)
    cls3 = dbscan(Tfidf_3gram_values)
    df['Tfidf_3gram_values']= Tfidf_3gram_values
    df['cls3']= cls3

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_4gram_values=tfidf_vectorizer.fit_transform(review4grams)
    cls4 = dbscan(Tfidf_4gram_values)
    df['Tfidf_4gram_values']= Tfidf_4gram_values
    df['cls4']= cls4
  
    df.to_csv('clustered'+appID+'.csv', encoding="utf-8")

"""##Apply TF-IDF on issue report"""

for appID in appIDs:
  path= '/content/'+appID+'.csv'

  with open(path) as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    #issueDF=pd.read_csv(csvfile)

    issueTitles = []
    issueNumbers=[]
    issueNumberOfComments=[]
    issueReporters=[]
    issueBodies = []
    cleanedIssueBody =[]
    issue2grams=[]
    issue3grams=[]
    issue4grams=[]

    for row in readCSV:
        issueBody = row[3]
        issueTitle = row[2]
        issueNumber=row[1]
        issueNumberOfComment= row[7]
        issueReporter= row[6]
        # clean issue body

        issueBody = str(issueBody)
        #Apply preprocessing functions
        #1
        result_removed_stopwords=remove_stopwords(issueBody)
        #2
        result_removed_punctuation=remove_punctuation(result_removed_stopwords)
        #3
        result_stemmed =stemmer(result_removed_punctuation)
        #4
        result_lemmatized=lemmatizing(result_stemmed)
        #5
        result_reduced_length= reduce_lengthening(result_lemmatized)
        #6
        result_typoCorrected= typoCorrection(result_reduced_length)
        #7
        result_removedNonEnglishAlphabets=remove_nonEnglishAlphabets(result_typoCorrected)
        #8
        result_remove_accented_chars = remove_accented_chars(result_removedNonEnglishAlphabets)
        #9
        result_issue_2grams = generate_ngrams(result_remove_accented_chars,2)
        result_issue_3grams = generate_ngrams(result_remove_accented_chars,3)
        result_issue_4grams = generate_ngrams(result_remove_accented_chars,4)

        result_issue_2grams= " ".join(result_issue_2grams)
        result_issue_3grams= " ".join(result_issue_3grams)
        result_issue_4grams= " ".join(result_issue_4grams)
        
        cleanedIssueBody.append(result_remove_accented_chars)
        issue2grams.append(result_issue_2grams)
        issue3grams.append(result_issue_3grams)
        issue4grams.append(result_issue_4grams)
        issueBodies.append(issueBody)
        issueNumbers.append(issueNumber)
        issueTitles.append(issueTitle)
        issueNumberOfComments.append(issueNumberOfComment)
        issueReporters.append(issueReporter)

        # compute TFIDF
    issueDF = pd.DataFrame()
    issueDF['issueNumber'] = issueNumbers
    issueDF['issueTitle']= issueTitles
    issueDF['issueBody']= issueBodies
    issueDF['issueReporter']= issueReporters
    issueDF['issueNumberOfComment']= issueNumberOfComments
    issueDF['issueReporter']= issueReporters
    issueDF['cleanedIssueBody'] = cleanedIssueBody
    issueDF['issue2grams']= issue2grams
    issueDF['issue3grams']= issue3grams
    issueDF['issue4grams']= issue4grams

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_issue_values=tfidf_vectorizer.fit_transform(cleanedIssueBody)
    issueDF['Tfidf_issue_values']= Tfidf_issue_values

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_issue2gram_values=tfidf_vectorizer.fit_transform(issue2grams)
    issueDF['Tfidf_issue2gram_values']= Tfidf_issue2gram_values

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_issue3gram_values=tfidf_vectorizer.fit_transform(issue3grams)
    issueDF['Tfidf_issue3gram_values']= Tfidf_issue3gram_values

    tfidf_vectorizer=TfidfVectorizer()
    Tfidf_issue4gram_values=tfidf_vectorizer.fit_transform(issue4grams)
    issueDF['Tfidf_issue4gram_values']= Tfidf_issue4gram_values
  
    issueDF.to_csv('TFIDFIssue'+appID+'.csv', encoding="utf-8")
    # settings that you use for count vectorizer will go here

"""##Compute cosine similarity between each document and all issue-report vectors"""

from scipy import spatial
import re

for appID in appIDs:
  path= '/content/clustered'+appID+'.csv'
  issuepath= '/content/TFIDFIssue'+appID+'.csv'

  with open(path) as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    dfReview=pd.read_csv(csvfile)

    dfReview['similarIssue']=None

    with open(issuepath) as issuecsvfile:
      issueReadCSV = csv.reader(issuecsvfile, delimiter=',')
      dfIssue=pd.read_csv(issuecsvfile)

      cosineBodies=[]
      cosine2grams=[]
      cosine3grams=[]
      cosine4grams=[]
      similarCluster=[]
      joinedBodies=[]
      
     
      #dfReview.groupby('cls')['cleanedBody'].apply(lambda cleanedBody: ','.join(cleanedBody))
      for key,group_df in dfReview.groupby('cls4'):
        #cosineBody = 1 - spatial.distance.cosine(group_df['Tfidf_values'], dfIssue['Tfidf_issue_values'])

        for ind in group_df.index:
          for indIssue in dfIssue.index:
            reviewtfidfstr= group_df['Tfidf_4gram_values'][ind]
            issuetfidfstr= dfIssue['Tfidf_issue4gram_values'][indIssue]
            issueNumber= dfIssue['issueNumber'][indIssue]

            reviewtfidf=re.split('\t|\n|',reviewtfidfstr)
            st= '  :'
            st2= ':'
            while st in reviewtfidf: reviewtfidf.remove(st) 
            while st2 in reviewtfidf: reviewtfidf.remove(st2)
            del reviewtfidf[0::2]
            for i in reviewtfidf:
              i=float(i)
            reviewtfidf=np.array(reviewtfidf)

            issuetfidf=re.split('\t|\n|',issuetfidfstr)
            #res = data.replace('_', ' ').replace(', ', ' ').split() 
            while st in issuetfidf: issuetfidf.remove(st) 
            while st2 in issuetfidf: issuetfidf.remove(st2)
            del issuetfidf[0::2]
            for i in issuetfidf:
              i=float(i)
            issuetfidf=np.array(issuetfidf)

            reviewtfidf = np.reshape(reviewtfidf, (-1, 1))
            issuetfidf = np.reshape(issuetfidf, (-1, 1))

            cosineBody= cosine_similarity(issuetfidf, reviewtfidf)
            #cosineBody = 1 - spatial.distance.cosine(issuetfidf, reviewtfidf)
            if (cosineBody.all() >= 0.85):
              group_df['similarIssue'][ind]= issueNumber
            else:
              group_df['similarIssue'][ind]= None
  
  dfReview.to_csv('Matches'+appID+'.csv', encoding="utf-8")
        
      

  """
      dfReview.groupby("cls2")['review2grams'].apply(lambda review2grams: ','.join(review2grams))
      dfReview.groupby("cls3")['review3grams'].apply(lambda review3grams: ','.join(review3grams))
      dfReview.groupby("cls4")['review4grams'].apply(lambda review4grams: ','.join(review4grams))

      for key,group_df in dfReview.groupby('cls2'):
        print("the group for product '{}' has {} rows".format(key,len(group_df)))  
        cosine2gram = 1 - spatial.distance.cosine(group_df['Tfidf_2gram_values'], dfIssue['Tfidf_issue2gram_values'])
        cosine2grams.append(cosine2gram)

      for key,group_df in dfReview.groupby('cls3'):
        print("the group for product '{}' has {} rows".format(key,len(group_df)))  
        cosine3gram = 1 - spatial.distance.cosine(group_df['Tfidf_3gram_values'], dfIssue['Tfidf_issue3gram_values'])
        cosine3grams.append(cosine3gram)

      for key,group_df in dfReview.groupby('cls4'):
        print("the group for product '{}' has {} rows".format(key,len(group_df)))  
        cosine4gram = 1 - spatial.distance.cosine(group_df['Tfidf_4gram_values'], dfIssue['Tfidf_issue4gram_values'])
        cosine3grams.append(cosine3gram)

   """

"""#Measuring user review metrics

###Senti-Strenth
"""

def tokenize_to_sentence(data):
  res = pd.DataFrame(columns=data.columns)
  for (_, r) in df_reviews.iterrows():
    review = tokenize.sent_tokenize(str(r['Body']))
    for sentence in review:
      tokenizeReview = r.copy()
      tokenizeReview['Body'] = sentence
      res = res.append(tokenizeReview)

  return res

#4 user review metrics- senti-strenth

#To capture the sentiment scores of user-reviews,we apply sentimental analysis on the user-reviews
# using the SentiStrength-SE tool

#Load necessary sentiStrength files
def load_senti_strength(dl = True):
  if (dl):
    url = 'https://www.dropbox.com/sh/7gecpg3y3iqicez/AAArw8FEvE0jwOjPlACdyFEPa?dl=0'
    !wget -O SentiStrength.zip $url
    !mkdir -p SentiStrength
    !cd SentiStrength && unzip -qo ../SentiStrength.zip -x /
    !rm SentiStrength.zip
load_senti_strength(True)

SentiStrengthLocation = "/content/SentiStrength/SentiStrengthCom.jar" #The location of SentiStrength
SentiStrengthLanguageFolder = "/content/SentiStrength/SentStrength_Data/" #The location of the unzipped SentiStrength data files

def rateSentiment(sentiString):
    p = subprocess.Popen(shlex.split("java -jar '" + SentiStrengthLocation + "' stdin sentidata '" + 
                                     SentiStrengthLanguageFolder + "'"),stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,stderr=subprocess.PIPE)
    b = bytes(sentiString.replace(" ","+"), 'utf-8')
    stdout_byte, stderr_text = p.communicate(b)
    stdout_text = stdout_byte.decode("utf-8")
    stdout_text = stdout_text.rstrip().replace("\t"," ")
    # get the positive sentiment and negative sentiment
    vals = stdout_text.split(' ')
    pos = int(vals[0])
    neg = int(vals[1])
    #return the dominent score as a sentence's sentiment score
    sentiment = neg if abs(neg) >= pos else pos
    return sentiment

"""##Calculating user-review metrics"""

import statistics

# The number of times that an issue is reported can affect its priority. 
# Developers may consider resolving an issue in the next release if the majority of users report the same issue.
# this metric is output of the clustering in RQ1
# I should add the calculations for this metric!!!!!

s=Statistics()
RatingArithmathy=[]
star_rating_neutral_sentiment_count=0
star_rating_high_sentiment_count=0
star_rating_law_sentiment_count=0
number_of_similar_user_reviews=[]
data=[]

reviewGQM=pd.DataFrame()

for appID in appIDs:
  reviewPath= '/content/clustered'+appID+'.csv'
  
  with open(reviewPath) as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    df=pd.read_csv(csvfile)

    sizes_of_user_reviews_sentence_mean=[]
    sizes_of_user_reviews_sentence_median=[]
    sizes_of_user_reviews_sentence_max=[]
    sizes_of_user_reviews_sentence_min=[]
    sizes_of_user_reviews_sentence_1q=[]
    sizes_of_user_reviews_sentence_3q=[]
    sizes_of_user_reviews_word_mean=[]
    sizes_of_user_reviews_word_median=[]
    sizes_of_user_reviews_word_min=[]
    sizes_of_user_reviews_word_max=[]
    sizes_of_user_reviews_word_1q=[]
    sizes_of_user_reviews_word_3q=[]
    sizes_of_user_reviews_title_word_mean=[]
    sizes_of_user_reviews_title_word_median=[]
    sizes_of_user_reviews_title_word_min=[]
    sizes_of_user_reviews_title_word_max=[]
    sizes_of_user_reviews_title_word_1q=[]
    sizes_of_user_reviews_title_word_3q=[]
    star_rating_neutral_sentiment=[]
    star_rating_high_sentiment=[]
    star_rating_law_sentiment=[]
    sentiment_score_mean=[]
    sentiment_score_median=[]
    sentiment_score_max=[]
    sentiment_score_min=[]
    sentiment_score_1q=[]
    sentiment_score_3q=[]
    proportion_of_negative_sentiment_score=[]
    proportion_of_positive_sentiment_score=[]
    proportion_of_neutral_sentiment_score=[]
    star_ratings_mean=[]
    star_ratings_median=[]
    star_ratings_min=[]
    star_ratings_max=[]
    star_ratings_1q=[]
    star_ratings_3q=[]
    number_of_similar_user_reviews=[]

    # here we are gouping the reviews based on the clusters
    # this is based on 4gram clusters we can change it to other types too!!!
    #for key,group_df in df.groupby('cls'):
    #for key,group_df in df.groupby('cls2'):
    #for key,group_df in df.groupby('cls3'):
    for key,group_df in df.groupby('cls4'):


# The size of a user-review can reflect the helpfulness and the importance of the user-review (Kim et al. 2006). 
# use Stanford parser (De Marneffe et al. 2006) to count the number of words and sentences in the user-reviews

        senBodySizes=[]
        wordBodySizes=[]
        wordTitleSizes=[]
        for (_, r) in group_df.iterrows():
          senBodySize = len(tokenize.sent_tokenize(str(r['Body'])))
          wordBodySize = len(tokenize.sent_tokenize(str(r['Body'])))
          senBodySizes.append(senBodySize)
          wordBodySizes.append(wordBodySize)
          wordTitleSize = len(tokenize.word_tokenize(str(r['Subject'])))
          wordTitleSizes.append(wordTitleSize)

        data= senBodySizes
        s.data = data
        data=data.sort()
        s.calculate()  

        senBodySizesArithmathy= [str(s.count), str(s.total), str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
                                 str(s.median), str(s.upper_quartile), str(s.maximum)]

        sizes_of_user_reviews_sentence_mean.append(senBodySizesArithmathy[2]) #sizes_of_user_reviews_sentence_mean
        sizes_of_user_reviews_sentence_median.append(senBodySizesArithmathy[5]) #sizes_of_user_reviews_sentence_median
        sizes_of_user_reviews_sentence_min.append(senBodySizesArithmathy[3]) #sizes_of_user_reviews_sentence_min
        sizes_of_user_reviews_sentence_max.append(senBodySizesArithmathy[7]) #sizes_of_user_reviews_sentence_max
        sizes_of_user_reviews_sentence_1q.append(senBodySizesArithmathy[4]) #sizes_of_user_reviews_sentence_1q
        sizes_of_user_reviews_sentence_3q.append(senBodySizesArithmathy[6]) #sizes_of_user_reviews_sentence_3q

        data= wordBodySizes
        s.data = data
        data=data.sort()
        s.calculate()  

        wordBodySizesArithmathy= [str(s.count), str(s.total), str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]

        sizes_of_user_reviews_word_mean.append(wordBodySizesArithmathy[2]) #sizes_of_user_reviews_word_mean
        sizes_of_user_reviews_word_median.append(wordBodySizesArithmathy[5]) #sizes_of_user_reviews_word_median
        sizes_of_user_reviews_word_min.append(wordBodySizesArithmathy[3]) #sizes_of_user_reviews_word_min
        sizes_of_user_reviews_word_max.append(wordBodySizesArithmathy[7]) #sizes_of_user_reviews_word_max
        sizes_of_user_reviews_word_1q.append(wordBodySizesArithmathy[4]) #sizes_of_user_reviews_word_1q
        sizes_of_user_reviews_word_3q.append(wordBodySizesArithmathy[6]) #sizes_of_user_reviews_word_3q

        ############ this is extra from the metrics defined by the authors ###########

        data= wordTitleSizes
        s.data = data
        data=data.sort()
        s.calculate() 

        wordTitleSizesArithmathy= [str(s.count), str(s.total), str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]

        sizes_of_user_reviews_title_word_mean.append(wordTitleSizesArithmathy[2]) #sizes_of_user_reviews_title_word_mean
        sizes_of_user_reviews_title_word_median.append(wordTitleSizesArithmathy[5]) #sizes_of_user_reviews_title_word_median
        sizes_of_user_reviews_title_word_min.append(wordTitleSizesArithmathy[3]) #sizes_of_user_reviews_title_word_min
        sizes_of_user_reviews_title_word_max.append(wordTitleSizesArithmathy[7]) #sizes_of_user_reviews_title_word_max
        sizes_of_user_reviews_title_word_1q.append(wordTitleSizesArithmathy[4]) #sizes_of_user_reviews_title_word_1q
        sizes_of_user_reviews_title_word_3q.append(wordTitleSizesArithmathy[6]) #sizes_of_user_reviews_title_word_3q
        
        data= group_df['Rating']

        for rating in group_df['Rating']:
          rating= int(rating)
          if rating<3:
            star_rating_law_sentiment_count=star_rating_law_sentiment_count+1
          elif rating>3:
            star_rating_high_sentiment_count=star_rating_high_sentiment_count+1
          else:
            star_rating_neutral_sentiment_count=star_rating_neutral_sentiment_count+1

          # To capture the diversity of ratings, we measure the proportion of negative, positive, and neutral user-reviews within each cluster.
          # Proportion of low, neutral, and positive star-ratings
          #consider a user-review with a star-rating equal to 3 as a neutral
          #if star_rating = 3:
        star_rating_neutral_sentiment.append(star_rating_neutral_sentiment_count/len(data)) #star_rating_neutral_sentiment

        #greater than 3 as a high
        #if star_rating > 3:
        star_rating_high_sentiment.append(star_rating_high_sentiment_count/len(data)) #star_rating_high_sentiment
    
        #less than 3 as a low user-review
        #if star_rating < 3:
        star_rating_law_sentiment.append(star_rating_law_sentiment_count/len(data)) #star_rating_law_sentiment
    
        #df_reviews = df_data[df_data['Rating'].between(0,3)]
        
        #####################################
        
        # The star-ratings do not always reflect the real sentiments of user-reviews
        # apply sentimental analysis on the user-reviews using the SentiStrength-SE tool (Islam and Zibran 2017).
        
        dataSentences=[]
        sentiments=[]

        sentencedf= tokenize_to_sentence(group_df)
        sentiments = sentencedf.apply (lambda row: rateSentiment(row['Body']), axis=1) 
        sentiments.fillna(0, inplace=True)
  
        data= sentiments
        neutral_sentiment_count=0
        high_sentiment_count=0
        law_sentiment_count=0
    
        for d in data:
          if -5<d<0:
            law_sentiment_count=law_sentiment_count+1
          elif 0<d<5:
            high_sentiment_count=high_sentiment_count+1
          else:
            neutral_sentiment_count=neutral_sentiment_count+1

        s.calculate() 

        sentimentArithmathy= [str(s.count), str(s.total), str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]

        sentiment_score_mean.append(sentimentArithmathy[2]) #sentiment_score_mean
        sentiment_score_median.append(sentimentArithmathy[5]) #sentiment_score_median
        sentiment_score_min.append(sentimentArithmathy[3]) #sentiment_score_min
        sentiment_score_max.append(sentimentArithmathy[7]) #sentiment_score_max
        sentiment_score_1q.append(sentimentArithmathy[4]) #sentiment_score_1q
        sentiment_score_3q.append(sentimentArithmathy[6]) #sentiment_score_3q

        # We measure the proportion of negative, positive, and neutral user-reviews in each cluster of user-reviews.
        proportion_of_negative_sentiment_score.append(law_sentiment_count/ len(data)) #proportion_of_negative_sentiment_score
        proportion_of_positive_sentiment_score.append(high_sentiment_count/ len(data)) #proportion_of_positive_sentiment_score
        proportion_of_neutral_sentiment_score.append(neutral_sentiment_count/ len(data)) #proportion_of_neutral_sentiment_score
        
        
        #############################################
        """
        group_df['Rating'].fillna(0, inplace=True)
        data= group_df['Rating']    
        data=data.sort_values()
        s.data = data
        s.calculate()      
  
        RatingArithmathy= [str(s.count), str(s.total), str(s.arithmetic_mean), str(s.minimum), str(s.lower_quartile),
             str(s.median), str(s.upper_quartile), str(s.maximum)]

        star_ratings_mean.append(RatingArithmathy[2])
        star_ratings_median.append(RatingArithmathy[5])
        star_ratings_min.append(RatingArithmathy[3])
        star_ratings_max.append(RatingArithmathy[7])
        star_ratings_1q.append(RatingArithmathy[4])
        star_ratings_3q.append(RatingArithmathy[6])
        """
       
        number_of_similar_user_reviews.append(len(group_df))
        
    reviewGQM = pd.DataFrame(number_of_similar_user_reviews, columns=['number_of_similar_user_reviews']) #number_of_similar_user_reviews= 0
    reviewGQM['sizes_of_user_reviews_sentence_mean']=  pd.Series(sizes_of_user_reviews_sentence_mean)
    reviewGQM['sizes_of_user_reviews_sentence_median']=  pd.Series(sizes_of_user_reviews_sentence_median)
    reviewGQM['sizes_of_user_reviews_sentence_min']=  pd.Series(sizes_of_user_reviews_sentence_min)
    reviewGQM['sizes_of_user_reviews_sentence_max']=  pd.Series(sizes_of_user_reviews_sentence_max)
    reviewGQM['sizes_of_user_reviews_sentence_1q']=  pd.Series(sizes_of_user_reviews_sentence_1q)
    reviewGQM['sizes_of_user_reviews_sentence_3q']=  pd.Series(sizes_of_user_reviews_sentence_3q)

    reviewGQM['sizes_of_user_reviews_word_mean']=  pd.Series(sizes_of_user_reviews_word_mean)
    reviewGQM['sizes_of_user_reviews_word_median']=  pd.Series(sizes_of_user_reviews_word_median)
    reviewGQM['sizes_of_user_reviews_word_min']=  pd.Series(sizes_of_user_reviews_word_min)
    reviewGQM['sizes_of_user_reviews_word_max']=  pd.Series(sizes_of_user_reviews_word_max)
    reviewGQM['sizes_of_user_reviews_word_1q']=  pd.Series(sizes_of_user_reviews_word_1q)
    reviewGQM['sizes_of_user_reviews_word_3q']=  pd.Series(sizes_of_user_reviews_word_3q)

    reviewGQM['sizes_of_user_reviews_title_word_mean']=  pd.Series(sizes_of_user_reviews_title_word_mean)
    reviewGQM['sizes_of_user_reviews_title_word_median']=  pd.Series(sizes_of_user_reviews_title_word_median)
    reviewGQM['sizes_of_user_reviews_title_word_min']=  pd.Series(sizes_of_user_reviews_title_word_min)
    reviewGQM['sizes_of_user_reviews_title_word_max']=  pd.Series(sizes_of_user_reviews_title_word_max)
    reviewGQM['sizes_of_user_reviews_title_word_1q']=  pd.Series(sizes_of_user_reviews_title_word_1q)
    reviewGQM['sizes_of_user_reviews_title_word_3q']=  pd.Series(sizes_of_user_reviews_title_word_3q)

    reviewGQM['star_rating_neutral_sentiment']=  pd.Series(star_rating_neutral_sentiment)
    reviewGQM['star_rating_high_sentiment']=  pd.Series(star_rating_high_sentiment)
    reviewGQM['star_rating_law_sentiment']=  pd.Series(star_rating_law_sentiment)

    reviewGQM['sentiment_score_mean']=  pd.Series(sentiment_score_mean)
    reviewGQM['sentiment_score_median']=  pd.Series(sentiment_score_median)
    reviewGQM['sentiment_score_min']=  pd.Series(sentiment_score_min)
    reviewGQM['sentiment_score_max']=  pd.Series(sentiment_score_max)
    reviewGQM['sentiment_score_1q']=  pd.Series(sentiment_score_1q)
    reviewGQM['sentiment_score_3q']=  pd.Series(sentiment_score_3q)

    reviewGQM['proportion_of_negative_sentiment_score']=  pd.Series(proportion_of_negative_sentiment_score)
    reviewGQM['proportion_of_positive_sentiment_score']=  pd.Series(proportion_of_positive_sentiment_score)
    reviewGQM['proportion_of_neutral_sentiment_score']=  pd.Series(proportion_of_neutral_sentiment_score)

    reviewGQM['star_ratings_mean'] = pd.Series(star_ratings_mean)
    reviewGQM['star_ratings_median']=  pd.Series(star_ratings_median)
    reviewGQM['star_ratings_min']=  pd.Series(star_ratings_min)
    reviewGQM['star_ratings_max']=  pd.Series(star_ratings_max)
    reviewGQM['star_ratings_1q']=  pd.Series(star_ratings_1q)
    reviewGQM['star_ratings_3q']=  pd.Series(star_ratings_3q)

  reviewGQM.to_csv('ReviewGQM'+appID+'.csv', sep=',', encoding='utf-8')

"""# RQ2

2. Model the issue reports prioritization using linear regression models

  2.1 dependent variable of the regression models is the prioritization orders of the apps

  2.2 independent variables are the metrics computed from both user-reviews and issue reports

    2.2.1  Identify the correlated variables: **Apply variable clustering analysis (Hmisc 2017) to build a hierarchical overview of the correlation between the independent metrics (Noei et al. 2017) * The metrics within each sub-hierarchy of metrics with Spearman’s |ρ| > 0.7 are considered as correlated variables (Nguyen et al. 2010). We choose one metric that is easier to comprehend for inclusion in our model from each sub-hierarchy of metrics


Build two types of regression models:

(i) Generic Model. We build a generic model using all of the subject apps. Building a generic regression model with a high goodness of fitness can show that different apps are following a similar strategy for prioritizing issue reports.

(ii) Specific Models. For each app, we build an independent regression model. We get a goodness of fitness for each independent regression model. A higher goodness of fitness can indicate that the issue reports prioritization of an app has a significant relationship with the metrics of user-reviews and issue reports. 

3. Calclulate R2 of the linear regression models: The goodness of fitness, i.e., adjusted R 2 (Nelder and Baker 1972), of the linear regression models shows whether issue reports prioritization has a relationship with the metrics of user-reviews and issue reports.
divide the apps into two groups; one group with R 2 ≥ 0.5 and another one with R 2 < 0.5

4. Compare the star-ratings of the two groups of apps using Mann–Whitney U test

5. Measure the effect size of the differences between star-ratings by applying Cliff’s δ (to make sure star rating in two groups are evenly distributed)
Cliff’s δ measures the degree of overlap between the two sets of star-ratings. Cliff’s δ measures the degree of overlap between the two sets of star-ratings. The output of the Cliff’s δ is a number between −1 and +1. If the distribution of star-ratings between the two sets of apps is identical, the Cliff’s δ would be 0 (Cliff 1993).

## Identify correlated variables

2.2.1 Identify the correlated variables

For this part varclus (which is part of harrel library is used)

source: https://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf

source code: https://cran.r-project.org/web/packages/Hmisc/index.html

**Issue**: the paper does not say exactly which metrics are chosen, just that the ones that are easier to comprehend for inclusion in their model.

varclus method is used to perform variable clustering (varclus) with a hierarchical structure. Varclus is a nice dimension reduction algorithm. Here is a short description:

A cluster is chosen for splitting.
The chosen cluster is split into two clusters by finding the first two principal components, performing an orthoblique rotation, and assigning each variable to the rotated component with which it has the higher squared correlation.

Variables are iteratively reassigned to clusters to maximize the variance accounted for by the cluster components.

Here Varclus is run on metrics of issue and review. the selected ones are amoung the ones with spearman > 0.7!
"""

issueGQM.info()

# 2.2.1 Identify the correlated variables

#for this part varclus (which is part of harrel library is used)
#source: https://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf
#source code: https://cran.r-project.org/web/packages/Hmisc/index.html

import pandas as pd
from varclushi import VarClusHi

review_vcs=[]
reviewVarcluss=[]
issue_vcs=[]
issueVarcluss=[]

for appID in appIDs:
  pathReview= '/content/ReviewGQM'+appID+'.csv'
#  pathIssue= '/content/'+app+'IssueGQM.csv'
  
  reviewGQM = pd.read_csv(pathReview, sep=';')
  review_vc = VarClusHi(reviewGQM,maxeigval2=1,maxclus=None)
  reviewVarcluss= review_vc.varclus()
  review_vcs.append(review_vc)

 # issueGQM = pd.read_csv(pathIssue, sep=';')
  issue_vc = VarClusHi(issueGQM,maxeigval2=1,maxclus=None)
  issueVarcluss= issue_vc.varclus()
  issue_vcs.append(issue_vc)

issueGQM.info()
reviewGQM.info()

review_vc.rsquare

issue_vc.rsquare

# source https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.spearmanr.html

from scipy.stats import spearmanr
correlatedVar=[]
# Create correlation matrix
corr_matrix = issueGQM.corr(method='spearman').abs()

# Select upper triangle of correlation matrix
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

# Find features with correlation greater than 0.95
to_drop= [column for column in corr_matrix.columns if any(corr_matrix[column]< 0.7)]
#to_drop = [column for column in upper.columns if any(upper[column] < 0.7)]
to_drop

#issueGQMCorrelated= issueGQM.drop(to_drop, axis=1, inplace=True)

print(corr_matrix)

corr_matrix.head(12)

# Drop features

"""##General Model- Model the issue reports prioritization using linear regression models & Calclulate R2 of the linear regression models

Build a generic model using all of the subject apps. Building a
generic regression model with a high goodness of fitness can show that different apps are following a similar strategy for prioritizing issue reports
"""

#2 Model the issue reports prioritization using linear regression models

#source: https://medium.com/analytics-vidhya/measuring-the-goodness-of-fit-r%C2%B2-versus-adjusted-r%C2%B2-1e8ed0b5784a
# import dataset

#data= pd.read_csv(path, encoding = "utf-8", delimiter=',')

data= issueGQM
  
issue_var= ['sizes_of_issue_reports_title_words', 'sizes_of_issue_reports_body_words', 'sizes_of_issue_reports_body_sentences', 'duration_reporter_join_github',
            'reporter_number_of_following', 'reporter_number_of_followers', 'number_of_reporter_gists_mean', 'number_of_public_repos_reporter', 'number_of_commnts']

# remove string and categorical variables
#cat_var = ['model', 'cyl', 'vs', 'am', 'gear', 'carb']

# scale the variables to prevent coefficients from becoming too large or too small
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data = scaler.fit_transform(data)



# fit the linear regression model to predict mpg as a function of other variables
from sklearn.linear_model import LinearRegression
reg = LinearRegression()
model = reg.fit(data[:, 1:8], data[:, 0])

# calculate r2 score
from sklearn.metrics import r2_score
r2 = r2_score(model.predict(data[:, 1:8]), data[:, 0])

# adjusted r2 using formula adj_r2 = 1 - (1- r2) * (n-1) / (n - k - 1)
# k = number of predictors = data.shape[1] - 1
adj_r2 = 1 - (1-r2)*(len(data) - 1) / (len(data) - (data.shape[1] - 1) - 1)
print(r2, adj_r2)

"""##Specific Model- Model the issue reports prioritization using linear regression models & Calclulate R2 of the linear regression models

For each app, build an independent regression model. Get a goodness of fitness for each independent regression model. A higher goodness of fitness can indicate that the issue reports prioritization of an app has a ignificant
relationship with the metrics of user-reviews and issue reports
"""

# Linear Regression Analysis fits a straight line between dependent variable and one or more independent variables.
# It is used for predicting the dependent variable using independent variables.

# 2.1 dependent variable of the regression models is the prioritization orders of the apps
# 2.2 independent variables are the metrics computed from both user-reviews and issue reports

# here we have 59 issue + 31 review independent variable (features)

#2 Model the issue reports prioritization using linear regression models

#source: https://medium.com/analytics-vidhya/measuring-the-goodness-of-fit-r%C2%B2-versus-adjusted-r%C2%B2-1e8ed0b5784a
# import dataset


from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
  
issue_var= ['sizes_of_issue_reports_title_words', 'sizes_of_issue_reports_body_words', 'sizes_of_issue_reports_body_sentences', 'duration_reporter_join_github',
            'reporter_number_of_following', 'reporter_number_of_followers', 'number_of_reporter_gists_mean', 'number_of_public_repos_reporter', 'number_of_commnts']

r2s=[]
adjR2s=[]
largerR2apps=[]
smallerR2apps=[]

for appID in appIDs:
  path= '/content/IssueGQM'+appID+'.csv'
  with open(path) as csvfile:
    readCSV = csv.reader(csvfile, delimiter=',')
    GQMIssue=pd.read_csv(csvfile)

    data= GQMIssue
    data.fillna(0, inplace=True)

    # scale the variables to prevent coefficients from becoming too large or too small
    scaler = MinMaxScaler()
    data = scaler.fit_transform(data)

    # fit the linear regression model to predict mpg as a function of other variables 
    reg = LinearRegression()
    model = reg.fit(data[:, 1:8], data[:, 0])

    # calculate r2 score
    r2 = r2_score(model.predict(data[:, 1:8]), data[:, 0])

    # adjusted r2 using formula adj_r2 = 1 - (1- r2) * (n-1) / (n - k - 1)
    # k = number of predictors = data.shape[1] - 1
    adj_r2 = 1 - (1-r2)*(len(data) - 1) / (len(data) - (data.shape[1] - 1) - 1)
    r2s.append(r2)
    adjR2s.append(adj_r2)

    if r2 < 0.5:
      smallerR2apps.append(appID)
    else:
      largerR2apps.append(appID)

"""##Compare the star-ratings of the two groups of apps using Mann–Whitney U test

In this part we were supposed to compute U-test between app with r2 larger than 0.5 and the ones with r2 smaller than 0.5
Since all of the apps have r2 smaller that 0.5 it is not possible to do this part, but anyway we have provided the related code. 

We have test this part of code manually and it works completely okay
"""

#4. Compare the star-ratings of the two groups of apps using Mann–Whitney U test
# Mann-Whitney U test
# input of mann-whitney is a two matrixes of average star ratings
# first matrix is average star ratings of first group of apps
# second matrix is average star ratings of second group of apps

# p − value for comarison is 0.05

smallerR2Rating=[]
largerR2Rating=[]

path= '/content/Replication.csv'

with open(path) as csvfile:
  readCSV = csv.reader(csvfile, delimiter=',')
  dfRating=pd.read_csv(csvfile)
  for row in readCSV:
    rating=row[1]
    for appID in appIDs:
      if appID in smallerR2apps:
        smallerR2Rating.append(rating)
      else:
        largerR2Rating.append(rating) 
# compare samples
stat, p = mannwhitneyu(smallerR2apps, largerR2Rating)
# interpretCliff’s 
alpha = 0.05
if p > alpha:
  print('Apps with R2 > 0.5:')
  print(largerR2apps)
else:
  print('Apps with R2 <= 0.5:')
  print(smallerR2apps)

"""##Measure the effect size of the differences between star-ratings by applying Cliff’s δ"""

#5. Measure the effect size of the differences between star-ratings by applying Cliff’s δ 

# source: https://github.com/ttumkaya/CliffDelta

# input of cliff's delta is two matrixes of average star ratings
# first matrix is average star ratings of first group of apps
# second matrix is average star ratings of second group of apps

import numpy as np

# Pass two "LISTS" to the function to calculate the difference in Cliff's delta.
def cliffDelta(x,y,decimals=2):
    lenx = len(x)
    leny = len(y)
    
    ## generate a matrix full of zeros
    matrix = np.zeros((lenx,leny))
    
    ## compare the two lists and put either 1 or -1 to the matrix (if they are equal, there is already a zero in the matrix) 
    for i in range(lenx):
        for j in range(leny):
            if x[i] > y[j]:
                matrix[i,j] = 1
            elif x[i] < y[j]:
                matrix[i,j] = -1
    
    ## get the avarage of the dominance matrix
    delta = matrix.mean()    
    return round(delta,decimals),matrix

# for the inputs here we should put the star ratings!!!!
delta, matrix = cliffDelta(smallerR2Rating,largerR2Rating)
delta

"""# RQ3

1. Built a random forest model to predict the prioritization levels of issue reports
2. Train a model with the issue reports of top N apps that have the highest star-ratings
3. Used the trained model to predict the prioritization levels of issue reports of the rest of the apps
4. Measured the accuracy of the predicted levels with the real levels using: Accuracy = Ic(i) /I(i)

5. Divide the apps in two groups based on prediction accuracy

*   Apps with the pair-wise similarity of accuracy score more than or equal to the threshold σ into one group
*   Apps with the pair-wise similarity of  accuracy score less than σ are placed into another group

6. Compared star ratings of these groups of apps (apps with more  using the Mann– Whitney U test than σ accuracy score and apps with less than σ accuracy score) 
7. Calculate the effect size of differences between the star-ratings by measuring Cliff’s δ

##Building a random forest model
"""

# 1. Built a random forest model to predict the prioritization levels of issue reports

# 3. Used the trained model to predict the prioritization levels of issue reports of the rest of the apps

# 4. Measured the accuracy of the predicted levels with the real levels using: Accuracy = Ic(i) /I(i)

accuracies=[]
lowerAccuracy=[]
higherAccuracy=[]
topAppIDs= {'app.easytoken'}

for topappID in topAppIDs:
  appIDs.remove(topappID)

path= '/content/IssueGQM'+appID+'.csv'

for appID in topAppIDs:
  with open(path) as csvfile:
    readCSV= csv.reader(csvfile, delimiter=',')
    topIssuedf= pd.concat([pd.read_csv(csvfile)])

topIssuedf = topIssuedf.notna()

X = topIssuedf[['sizes_of_issue_reports_title_words', 'sizes_of_issue_reports_body_words', 
                'sizes_of_issue_reports_body_sentences', 'duration_reporter_join_github', 'reporter_number_of_following', 
                'reporter_number_of_followers', 'number_of_reporter_gists_mean', 'number_of_public_repos_reporter']].values

y= topIssuedf.iloc[:, 9]

for appID in appIDs:
  with open(path) as csvfile:
    readCSV= csv.reader(csvfile, delimiter=',')
    issuedf= pd.read_csv(csvfile)
    
    issuedf = issuedf.notna()

    ## Taking care of missing data (part2)
    from sklearn.impute import SimpleImputer
    missingvalues = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = 0)
    missingvalues = missingvalues.fit(X[:, 3:8])
    X[:, 3:8]=missingvalues.transform(X[:, 3:8])

    ## Splitting the dataset into the Training set and Test set
    from sklearn.model_selection import train_test_split
    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)

    X_train= X
    y_train= y

    X_test= issuedf[['sizes_of_issue_reports_title_words', 'sizes_of_issue_reports_body_words', 
                     'sizes_of_issue_reports_body_sentences', 'duration_reporter_join_github', 'reporter_number_of_following', 
                     'reporter_number_of_followers', 'number_of_reporter_gists_mean', 'number_of_public_repos_reporter']].values

    y_test= issuedf.iloc[:, 9]

    ## Fitting Classifier to the Training set
    from sklearn.ensemble import RandomForestClassifier
    classifier = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0)
    #classifier.fit(x_train,y_train)
    classifier.fit(X_train,np.ravel(y_train,order='C'))

    ##Predicting the test set result
    y_pred = classifier.predict(X_test)

    ##Making Confusion matrix
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test,y_pred)

    ##Calculate accuracy
    from sklearn.metrics import accuracy_score
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)

    if accuracy < 0.5:
      lowerAccuracy.append(appID)
    else:
      higherAccuracy.append(appID)

# 5. Divide the apps in two groups based on prediction accuracy

# divide the test apps into two groups based on the accuracy of the predicted levels

# pair-wise similarity of more than or equal to the threshold σ into one group
# pair-wise similarity of less than the threshold σ into another group

"""##Compare star ratings of groups of apps using the Mann– Whitney U test

**6. Compared star ratings of these groups of apps using the Mann– Whitney U test**

Source: https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/


**Mann-Whitney U test**
from numpy.random import seed
from numpy.random import randn
from scipy.stats import mannwhitneyu

**seed the random number generator**
seed(1)

**generate two independent samples**
data1 = 5 * randn(100) + 50
data2 = 5 * randn(100) + 51

**compare samples**
stat, p = mannwhitneyu(data1, data2)
print('Statistics=%.3f, p=%.3f' % (stat, p))

**interpret**
alpha = 0.05
if p > alpha:
	print('Same distribution (fail to reject H0)')
else:
	print('Different distribution (reject H0)')
1
"""

#4. Compare the star-ratings of the two groups of apps using Mann–Whitney U test
# Mann-Whitney U test
# input of mann-whitney is a two matrixes of average star ratings
# first matrix is average star ratings of first group of apps
# second matrix is average star ratings of second group of apps

# p − value for comarison is 0.05


lowerAccuracyRating=[]
higherAccuracyRating=[]

path= '/content/Replication.csv'
with open(path) as csvfile:
  readCSV = csv.reader(csvfile, delimiter=',')
  dfRating=pd.read_csv(csvfile)
  for row in readCSV:
    rating=row[1]
    for appID in appIDs:
      if appID in lowerAccuracy:
        lowerAccuracyRating.append(rating)
      else:
        higherAccuracyRating.append(rating) 

# compare samples
stat, p = mannwhitneyu(lowerAccuracyRating, higherAccuracyRating)

# interpretCliff’s 
alpha = 0.05
if p > alpha:
	print('Apps with higher accuracy> 0.5:')
  print(higherAccuracyRating)
else:
	print('Apps with lower accuracy <= 0.5:')
  print(lowerAccuracyRating)

"""##Calculate the effect size by Cliff’s δ"""

#5. Measure the effect size of the differences between star-ratings by applying Cliff’s δ 

# source: https://github.com/ttumkaya/CliffDelta

# input of cliff's delta is two matrixes of average star ratings
# first matrix is average star ratings of first group of apps
# second matrix is average star ratings of second group of apps

import numpy as np

# Pass two "LISTS" to the function to calculate the difference in Cliff's delta.
def cliffDelta(x,y,decimals=2):
    lenx = len(x)
    leny = len(y)
    
    ## generate a matrix full of zeros
    matrix = np.zeros((lenx,leny))
    
    ## compare the two lists and put either 1 or -1 to the matrix (if they are equal, there is already a zero in the matrix) 
    for i in range(lenx):
        for j in range(leny):
            if x[i] > y[j]:
                matrix[i,j] = 1
            elif x[i] < y[j]:
                matrix[i,j] = -1
    
    ## get the avarage of the dominance matrix
    delta = matrix.mean()    
    return round(delta,decimals),matrix

# for the inputs here we should put the star ratings!!!!
delta, matrix = cliffDelta(lowerAccuracyRating,higherAccuracyRating)
delta